{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### install necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in ./.venv/lib/python3.12/site-packages (2024.12.4)\n",
      "Found existing installation: unsloth 2024.12.4\n",
      "Uninstalling unsloth-2024.12.4:\n",
      "  Successfully uninstalled unsloth-2024.12.4\n",
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-kjbzjhpt/unsloth_15ef7430fc9c4ab3bd391f5c9a0c2b34\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-kjbzjhpt/unsloth_15ef7430fc9c4ab3bd391f5c9a0c2b34\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 85f1fa096afde5efe2fb8521d8ceec8d13a00715\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: unsloth_zoo>=2024.11.8 in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.12.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
      "Requirement already satisfied: tyro in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.2)\n",
      "Requirement already satisfied: transformers>=4.46.1 in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.47.1)\n",
      "Requirement already satisfied: datasets>=2.16.0 in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.27.0)\n",
      "Requirement already satisfied: hf_transfer in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
      "Requirement already satisfied: bitsandbytes>=0.43.3 in ./.venv/lib/python3.12/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.5.1)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
      "Requirement already satisfied: triton in ./.venv/lib/python3.12/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in ./.venv/lib/python3.12/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.2.1)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in ./.venv/lib/python3.12/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.0)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in ./.venv/lib/python3.12/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.0)\n",
      "Requirement already satisfied: cut_cross_entropy in ./.venv/lib/python3.12/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.12.2)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from unsloth_zoo>=2024.11.8->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.0.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./.venv/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./.venv/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./.venv/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./.venv/lib/python3.12/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.12.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.12.4-py3-none-any.whl size=173746 sha256=60c0330a88e0b9e62e4a41354ae43bbb1b31b58f1d51c6d57fdb77c7d55ee963\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-sj3_26z3/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.12.4\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fozle/Projects/Personalized-LLM/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.61 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load fine-tuned model  \n",
    "ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"./ft_model/1B_finetuned_llama3.2\",\n",
    "    max_seq_length=5020,\n",
    "    load_in_4bit=True, \n",
    "    dtype=None\n",
    ")\n",
    "ft_model = FastLanguageModel.for_inference(ft_model)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear CUDA cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, text, data_prompt):\n",
    "    clear_memory()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            [data_prompt.format(input=text)],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=5020,  # Keeping the original parameter\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "            # streamer=TextStreamer(tokenizer)\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.batch_decode(outputs)[0]\n",
    "        cleaned_response = response.split(\"Response:\")[-1].strip()\n",
    "        \n",
    "        # Remove unwanted tokens\n",
    "        cleaned_response = cleaned_response.replace(\"<|begin_of_text|>\", \"\").replace(\"<|end_of_text|>\", \"\").strip()\n",
    "        \n",
    "    clear_memory()\n",
    "    return cleaned_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prompt = \"\"\"Analyze the mental health aspects in this text:\n",
    "{input}\n",
    "Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't say for sure but based on your description of how you've felt all your life and have experienced as a young adult and still experience today what is new is feeling like you've never tried or contemplated suicide. Since suicide is a serious decision and because it's illegal, I'm wondering if talking about your feelings and what's going on right now would be helpful? Maybe an appropriate counselor can point you in the right direction.\n"
     ]
    }
   ],
   "source": [
    "text = \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here. I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it. How can I change my feeling of being worthless to everyone?\"\n",
    "\n",
    "response = generate_response(ft_model, ft_tokenizer, text, data_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_json(\"hf://datasets/Amod/mental_health_counseling_conversations/combined_dataset.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (696, 2)\n",
      "\n",
      "First few rows:\n",
      "                                             Context  \\\n",
      "0  Any time my family and friends are in an alter...   \n",
      "1  I've only been married three months. Every wee...   \n",
      "2  I was in a car accident and totaled my car. I ...   \n",
      "3  My dad is doing some really bad drugs, and I'm...   \n",
      "4  My daughter is in later elementary school. She...   \n",
      "\n",
      "                                            Response  \n",
      "0  Breaking the patterns of relating to family me...  \n",
      "1  At the begin g of marriages, we expect to be l...  \n",
      "2  When we feel overwhelmed by life situations, i...  \n",
      "3  It seems like you are going trough stages of g...  \n",
      "4  Depending on your daughter’s age, this could b...  \n"
     ]
    }
   ],
   "source": [
    "# Load dataset from CSV file\n",
    "dataset = pd.read_csv(\"./dataset/test_mental_health.csv\")\n",
    "\n",
    "# Select only Context and Response columns\n",
    "test_dataset = dataset[['Context', 'Response']]\n",
    "\n",
    "# Display first few rows to verify\n",
    "print(\"Dataset shape:\", test_dataset.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(test_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score(model, tokenizer, dataset, data_prompt):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        context = row['Context']\n",
    "        actual_response = row['Response']\n",
    "        \n",
    "        generated_response = generate_response(model, tokenizer, context, data_prompt)\n",
    "        \n",
    "        y_true.append(actual_response)\n",
    "        y_pred.append(generated_response)\n",
    "    \n",
    "    # Preprocess responses for F1 score calculation\n",
    "    vectorizer = CountVectorizer().fit(y_true + y_pred)\n",
    "    y_true_vectors = vectorizer.transform(y_true).toarray()\n",
    "    y_pred_vectors = vectorizer.transform(y_pred).toarray()\n",
    "    \n",
    "    # Calculate F1 score for each word and average them\n",
    "    f1_scores = []\n",
    "    for i in range(y_true_vectors.shape[1]):\n",
    "        f1 = f1_score(y_true_vectors[:, i], y_pred_vectors[:, i], average='weighted', zero_division=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6863\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate F1 score\n",
    "f1 = calculate_f1_score(ft_model, ft_tokenizer, test_dataset, data_prompt)\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.61 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load original model\n",
    "base_model, base_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "    max_seq_length=5020,\n",
    "    load_in_4bit=True, \n",
    "    dtype=None\n",
    ")\n",
    "base_model = FastLanguageModel.for_inference(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Original Model: 0.6994\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score with original model\n",
    "f1_base = calculate_f1_score(base_model, base_tokenizer, test_dataset, data_prompt)\n",
    "print(f\"F1 Score with Original Model: {f1_base:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_shot_prompt(examples, k, query):\n",
    "    prompt = \"\"\n",
    "    for i in range(k):\n",
    "        prompt += f\"Example {i+1}:\\nContext: {examples.iloc[i]['Context']}\\nResponse: {examples.iloc[i]['Response']}\\n\\n\"\n",
    "    prompt += f\"Now analyze this:\\nContext: {query}\\nResponse:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_shot_response(model, tokenizer, text, data_prompt, examples, k):\n",
    "    clear_memory()\n",
    "    \n",
    "    prompt = create_k_shot_prompt(examples, k, text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            [prompt],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,  # Adjusted for faster inference\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.batch_decode(outputs)[0]\n",
    "        cleaned_response = response.split(\"Response:\")[-1].strip()\n",
    "        \n",
    "        # Remove unwanted tokens\n",
    "        cleaned_response = cleaned_response.replace(\"<|begin_of_text|>\", \"\").replace(\"<|end_of_text|>\", \"\").strip()\n",
    "        \n",
    "    clear_memory()\n",
    "    return cleaned_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_k_shot_f1_score(model, tokenizer, dataset, data_prompt, k):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    examples = dataset.head(k)  # Use the first k examples for prompting\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        context = row['Context']\n",
    "        actual_response = row['Response']\n",
    "        \n",
    "        generated_response = generate_k_shot_response(model, tokenizer, context, data_prompt, examples, k)\n",
    "        \n",
    "        y_true.append(actual_response)\n",
    "        y_pred.append(generated_response)\n",
    "    \n",
    "    # Preprocess responses for F1 score calculation\n",
    "    vectorizer = CountVectorizer().fit(y_true + y_pred)\n",
    "    y_true_vectors = vectorizer.transform(y_true).toarray()\n",
    "    y_pred_vectors = vectorizer.transform(y_pred).toarray()\n",
    "    \n",
    "    # Calculate F1 score for each word and average them\n",
    "    f1_scores = []\n",
    "    for i in range(y_true_vectors.shape[1]):\n",
    "        f1 = f1_score(y_true_vectors[:, i], y_pred_vectors[:, i], average='weighted', zero_division=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Fine-Tuned Model (K-Shot): 0.6834\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score with k-shot prompting using the fine-tuned model\n",
    "k = 1  # Number of examples to include in the prompt\n",
    "f1_k_shot = calculate_k_shot_f1_score(ft_model, ft_tokenizer, test_dataset, data_prompt, k)\n",
    "print(f\"F1 Score with Fine-Tuned Model (K-Shot): {f1_k_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Fine-Tuned Model (K-Shot): 0.7031\n"
     ]
    }
   ],
   "source": [
    "k = 2  # Number of examples to include in the prompt\n",
    "f1_k_shot = calculate_k_shot_f1_score(ft_model, ft_tokenizer, test_dataset, data_prompt, k)\n",
    "print(f\"F1 Score with Fine-Tuned Model (K-Shot): {f1_k_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Fine-Tuned Model (K-Shot): 0.7117\n"
     ]
    }
   ],
   "source": [
    "k = 3  # Number of examples to include in the prompt\n",
    "f1_k_shot = calculate_k_shot_f1_score(ft_model, ft_tokenizer, test_dataset, data_prompt, k)\n",
    "print(f\"F1 Score with Fine-Tuned Model (K-Shot): {f1_k_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Base Model (K-Shot): 0.6657\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score with k-shot prompting using original model\n",
    "k = 1  # Number of examples to include in the prompt\n",
    "f1_k_shot = calculate_k_shot_f1_score(base_model, base_tokenizer, test_dataset, data_prompt, k)\n",
    "print(f\"F1 Score with Base Model (K-Shot): {f1_k_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Base Model (K-Shot): 0.6845\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score with k-shot prompting using original model\n",
    "k = 2  # Number of examples to include in the prompt\n",
    "f1_k_shot = calculate_k_shot_f1_score(base_model, base_tokenizer, test_dataset, data_prompt, k)\n",
    "print(f\"F1 Score with Base Model (K-Shot): {f1_k_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score with Base Model (K-Shot): 0.6799\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score with k-shot prompting using original model\n",
    "k = 3  # Number of examples to include in the prompt\n",
    "f1_k_shot = calculate_k_shot_f1_score(base_model, base_tokenizer, test_dataset, data_prompt, k)\n",
    "print(f\"F1 Score with Base Model (K-Shot): {f1_k_shot:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking GPT3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
      "Using cached anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Downloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.7.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.58.1 pydantic-2.10.4 pydantic-core-2.27.2 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt_response(text, data_prompt):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Analyze mental health aspects in the given text.\"},\n",
    "                {\"role\": \"user\", \"content\": data_prompt.format(input=text)}\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gpt_f1_score(dataset, data_prompt):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        context = row['Context']\n",
    "        actual_response = row['Response']\n",
    "        generated_response = generate_gpt_response(context, data_prompt)\n",
    "        \n",
    "        y_true.append(actual_response)\n",
    "        y_pred.append(generated_response)\n",
    "    \n",
    "    vectorizer = CountVectorizer().fit(y_true + y_pred)\n",
    "    y_true_vectors = vectorizer.transform(y_true).toarray()\n",
    "    y_pred_vectors = vectorizer.transform(y_pred).toarray()\n",
    "    \n",
    "    f1_scores = []\n",
    "    for i in range(y_true_vectors.shape[1]):\n",
    "        f1 = f1_score(y_true_vectors[:, i], y_pred_vectors[:, i], average='weighted', zero_division=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5 Turbo F1 Score: 0.6691\n"
     ]
    }
   ],
   "source": [
    "# Calculate GPT-3.5 F1 Score\n",
    "gpt_f1 = calculate_gpt_f1_score(test_dataset, data_prompt)\n",
    "print(f\"GPT-3.5 Turbo F1 Score: {gpt_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 3.5 K-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt_k_shot_response(text, examples, k):\n",
    "    try:\n",
    "        k_shot_prompt = create_k_shot_prompt(examples, k, text)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a mental health expert analyzing text.\"},\n",
    "                {\"role\": \"user\", \"content\": k_shot_prompt}\n",
    "            ],\n",
    "            max_tokens=512,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        time.sleep(1)\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gpt_k_shot_f1(dataset, k):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    examples = dataset.head(k)  # Get k examples for prompting\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        generated_response = generate_gpt_k_shot_response(row['Context'], examples, k)\n",
    "        y_true.append(row['Response'])\n",
    "        y_pred.append(generated_response)\n",
    "    \n",
    "    vectorizer = CountVectorizer().fit(y_true + y_pred)\n",
    "    y_true_vectors = vectorizer.transform(y_true).toarray()\n",
    "    y_pred_vectors = vectorizer.transform(y_pred).toarray()\n",
    "    \n",
    "    f1_scores = []\n",
    "    for i in range(y_true_vectors.shape[1]):\n",
    "        f1 = f1_score(y_true_vectors[:, i], y_pred_vectors[:, i], average='weighted', zero_division=1)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5 Turbo 1-Shot F1 Score: 0.6740\n"
     ]
    }
   ],
   "source": [
    "# Calculate k-shot F1 score\n",
    "k = 1  # Number of examples\n",
    "gpt_k_shot_f1 = calculate_gpt_k_shot_f1(test_dataset, k)\n",
    "print(f\"GPT-3.5 Turbo {k}-Shot F1 Score: {gpt_k_shot_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5 Turbo 2-Shot F1 Score: 0.6799\n"
     ]
    }
   ],
   "source": [
    "# Calculate k-shot F1 score\n",
    "k = 2  # Number of examples\n",
    "gpt_k_shot_f1 = calculate_gpt_k_shot_f1(test_dataset, k)\n",
    "print(f\"GPT-3.5 Turbo {k}-Shot F1 Score: {gpt_k_shot_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3.5 Turbo 3-Shot F1 Score: 0.6765\n"
     ]
    }
   ],
   "source": [
    "# Calculate k-shot F1 score\n",
    "k = 3  # Number of examples\n",
    "gpt_k_shot_f1 = calculate_gpt_k_shot_f1(test_dataset, k)\n",
    "print(f\"GPT-3.5 Turbo {k}-Shot F1 Score: {gpt_k_shot_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
